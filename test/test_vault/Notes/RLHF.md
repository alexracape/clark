> Reinforcement learning from human feedback

Uses [[Reinforcement Learning]] to align a large language model with human preferences

Optimization techniques include [[PPO]], [[DPO]], and [[GRPO]]